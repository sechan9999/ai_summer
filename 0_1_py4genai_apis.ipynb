{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2qwqjTZ3TwcA"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPUH9z52aIcnDIGLOg6/t8N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanderbilt-data-science/ai_summer/blob/main/0_1_py4genai_apis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions and APIs\n",
        "> Expanding things Python \"Already Knows\"\n",
        "\n",
        "In the last lesson, we learned about Google Colab, Python, and built-in data types and data structures. We talked about how there are some tasks/instructions/data that Python already knows how to do, and then others which we need to tell Python about. Today, we'll learn the syntax and grammar of how to communicate higher-order tasks to Python.\n",
        "\n",
        "## Lesson Objectives\n",
        "At the end of today's lesson, you should be able to:\n",
        "* Describe the purpose of a function\n",
        "* Describe the different input/output relationships of functions\n",
        "* Write a function\n",
        "* Describe what it means to install packages/modules/libraries\n",
        "* Describe what it means to import packages/modules/libraries\n",
        "* Use an API to accomplish a task\n",
        "\n",
        "Let's get started!"
      ],
      "metadata": {
        "id": "mMBdFRMzYuea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings"
      ],
      "metadata": {
        "id": "GcmgT2ia1UQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are functions?\n",
        "\n",
        "A function is essentially a programming apparatus which:\n",
        "* Optionally receives some data as inputs\n",
        "* Performs some operation\n",
        "* Optionally returns some values\n",
        "\n",
        "What? \n",
        "\n",
        "One can conceptualize a function to things we do and interact with in every day life.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Piqsels.com-id-zbxec.jpg/640px-Piqsels.com-id-zbxec.jpg\" width=\"300\">\n",
        "</center>\n",
        "\n",
        "Describe how we've been using the Generative AI websites to chat with our models."
      ],
      "metadata": {
        "id": "42ZeCQ35Zzy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Writing a Chat Function\n",
        "#@markdown Let's begin to codify how one might write an chat function.\n",
        "\n",
        "#@markdown **Description.** What will the chat function do (in plain words)?\n",
        "chat_description = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown **Function Name.** What would you like to call this chat function? It should be descriptive but concise.\n",
        "function_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown **Parameters.** What things affect how the chat behaves?\n",
        "chat_input_parameters = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown **Main object.** What are the main inputs that the chat will be manipulating?\n",
        "chat_main_object = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown **Outputs.** What is the output of the chat?\n",
        "chat_outputs = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown **Chat Behavior.** What does the chat itself need to do generate the outputs? You can write something short,\n",
        "#@markdown and broad, given that we will actually write this later.\n",
        "chat_behavior = \"\" #@param {type:\"string\"}\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "l141Ri1lYRR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you've filled out the previous form, you've got all the components necessary to actually write a chat function. Now, you just need the syntax and grammar of how to communicate it to Python. The syntax for a function looks like this, using your defined parameters above:\n",
        "\n",
        "```\n",
        "# def, paretheses, commas, and colon are syntax elements to communicate a function\n",
        "def function_name(chat_main_object, chat_input_parameters):\n",
        "  '''\n",
        "  chat_description\n",
        "  '''\n",
        "  \n",
        "  #lines of code operating on the oven_main_object and using the chat_input_parameters, note indentation\n",
        "  chat_behavior\n",
        "\n",
        "  #'return' keyword communicates the object(s) to be returned\n",
        "  return chat_outputs\n",
        "```\n",
        "\n",
        "Let's try this with code...."
      ],
      "metadata": {
        "id": "3802ZQhVYRSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Breakout Room: Chat as a Function (10 minutes)\n",
        "In this breakout room, you'll use your generative AI of choice to create this function. The goal is to create _the simplest reasonable function with no unnecessary elements_.\n",
        "\n",
        "**Tasks to execute**:\n",
        "1. Generate the chat function given the functionality described above. The model type you will use will be a Huggingface transformer. Record your prompt and any necessary extra prompts to simplify the response.\n",
        "2. Verify that the function behaves as expected. What steps did you take to do this? You can use `gpt2` as the model name.\n",
        "3. Were any unnecessary code elements generated? If so, what were they?\n",
        "4. (If time) Remove the warning for pad_token_id.\n",
        "\n",
        "Add your code in new cells or the cells provided below.\n",
        "\n",
        "**Hints**\n",
        "<details>\n",
        "<summary>Hints to help with unsuccessful prompts</summary>\n",
        "\n",
        "The following prompt may give you a good starting point:\n",
        "<blockquote>\n",
        "\"Can you write a function in the simplest, clearest way possible in Python called get_chat? It receives as input an huggingface transformer model name and the text to be sent to the model. The function should return the response.\"\n",
        "</blockquote>\n",
        "\n",
        "<blockquote>\n",
        "Can you modify the function so that if I don't pass in a model name, the function still runs the pipeline and generates the response?\n",
        "</blockquote>\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>Hints to help with transformers</summary>\n",
        "\n",
        "1. You will most likely want to use the `pipeline` abstraction to implement the model functionality.\n",
        "2. If you use `pipeline`, you'll also want to use `text-generation` as the type.\n",
        "</details>"
      ],
      "metadata": {
        "id": "FIy7wVD7h6ct"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-VOpXwg0qMWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cfMoscsLrePM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PtS2uNWmpeS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Default parameters and keywords\n",
        "Here, we had two inputs, but these models actually have loonngg lists of ways that you can change the behavior of the model (or pipeline). You can see an example of some of these parameters for text generation pipelines for Huggingface [in their documentation.](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline.__call__)\n",
        "\n",
        "What if we wanted to add 2 parameters to:\n",
        "1. Set the device (CPU vs GPU), and \n",
        "2. Choose whether we wanted to return full text or not? Let's see how we can do this.\n",
        "\n",
        "What if additionally, we wanted text to be a required input, whereas the rest of the parameters could be optional? How would this change our function? We will assume that ChatGPT gave us the following response, and we'll test it out!\n",
        "\n",
        "```\n",
        "def get_chat(text, model_name=None, device=-1, return_full_text=False):\n",
        "    generator = pipeline(\"text-generation\", model=model_name, device=device, model_kwargs={\"pad_token_id\": 50256})\n",
        "    response = generator(text, return_full_text=return_full_text)[0]['generated_text']\n",
        "    return response\n",
        "```\n"
      ],
      "metadata": {
        "id": "jkiF_FxJz447"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FSVhS74bz4fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check to see if we can run with just the required input\n",
        "get_chat('The cat ran down the hall! It was crazy!! And then, it')"
      ],
      "metadata": {
        "id": "kp7tdViI0X4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check to see whether we can run without the required input\n"
      ],
      "metadata": {
        "id": "D5k_5xAi28Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check to see if order matters\n",
        "get_chat('The dog might have rolled in garbage juice this morning, but', return_full_text=True, model_name='gpt2')"
      ],
      "metadata": {
        "id": "kSFL0qsF3A-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check to see if order matters with named required inputs\n"
      ],
      "metadata": {
        "id": "U8WFIp1O3c_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check to see if order matters with unnamed required inputs\n"
      ],
      "metadata": {
        "id": "do2V1gk03rYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Catching unwanted behavior\n",
        "Sometimes, it will be the case that you want to make sure that certain things don't happen in your code, or at least be notified that they have occurred.\n",
        "\n",
        "This can be achieved through:\n",
        "\n",
        "1. Warnings: continue code execution, but notify the user\n",
        "2. Exceptions: halt code execution and notify the user\n",
        "\n",
        "You can check out the inverse of this (i.e., suppressing warnings - not usually a good idea) through the warnings package."
      ],
      "metadata": {
        "id": "fHSmhug-4U0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utilizing example above\n",
        "with warnings.catch_warnings():\n",
        "  warnings.simplefilter(\"ignore\")\n",
        "  display(get_chat('The dog might have rolled in garbage juice this morning, but', return_full_text=True, model_name='gpt2'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "n9C_ZxNp4jz-",
        "outputId": "8bc9ca3a-54c6-4dab-91b3-ab91d834d60e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"The dog might have rolled in garbage juice this morning, but did that really mean it made poop? If no poop, I don't know where to put it. A few dogs with poop found the bag full of poop at a friend's house,\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fantastic. We learned that a function:\n",
        "* does something\n",
        "* can have inputs\n",
        "* can have outputs\n",
        "* should be defined in memory\n",
        "* must be called on something to operate\n",
        "\n",
        "**See homework for investigation of advanced concepts.**"
      ],
      "metadata": {
        "id": "x9v9jqGdyhXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object-oriented Programming"
      ],
      "metadata": {
        "id": "GALCY1t-dlYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions, Methods, and Classes\n",
        "We just now wrote our own function, but Python is chocked full of lots of different built-in functions that you can already use! We just now used some:\n",
        "* `print()`\n",
        "* `lower()`\n",
        "\n",
        "And saw several others in Python's API for math operations and strings.\n",
        "\n",
        "Most strictly speaking, the code we just wrote was a function, but when we call \"operations\" relevant to the object they're working on, this is called a _method_. This distinction matters to the extent to which you can capitalize upon this knowledge.\n",
        "\n",
        "What? I know, that was a confusing statement, so lets take an example.\n",
        "\n",
        "<center>\n",
        "<figure>\n",
        "<img src=\"https://github.com/vanderbilt-data-science/p4ai-essentials/blob/main/img/oven_class.png?raw=true\" width=\"600\"/>\n",
        "<figcaption>Not all methods should apply to all objects. Sometimes, it helps for the specification of a particular object to come grouped with its attributes as well.</figcaption>\n",
        "</figure>\n",
        "</center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TCtSVP_yxWmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A more relevant example: LLM classes\n",
        "\n",
        "Suppose we decide that we don't want to view our `chat` functionality as a function. We decide to view it as a `class`, because we can use Huggingface Models, OpenAI models, Bard, Claude, etc. So we decide that instead, we will create a class for each of these different model types.\n",
        "\n",
        "The first step here is essentially to design and implement the classes. This will give us the tools to implement higher and higher levels of functionality. Keep in your mind that we're trying to identify:\n",
        "- Attributes or characteristics of the model that we want to set or are useful to know\n",
        "- Methods of interacting with the model\n",
        "\n"
      ],
      "metadata": {
        "id": "6AyeXHOjBIR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "\n",
        "**1. What functionality does each model type need to perform?**\n",
        "*  \n",
        "\n",
        "**2. Does the functionality differ for `get_chat` for the different models?**\n",
        "*  \n",
        "\n",
        "**3. What functionality is best left performed one time?**\n",
        "*  \n",
        "\n",
        "**4. What characteristics should be stored about each model?**\n",
        "*  \n",
        "\n",
        "**5. Speaking generally, what is common about these models?**\n",
        "*  "
      ],
      "metadata": {
        "id": "IE3uy0hVD30h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class syntax:\n",
        "\n",
        "```\n",
        "# def, paretheses, commas, and colon are syntax elements to communicate a function\n",
        "def class_name(ParentClass):\n",
        "  '''\n",
        "  Class description\n",
        "  '''\n",
        "  \n",
        "  #initialization function\n",
        "  def __init__(self, init_input, init_attribute):\n",
        "    self.attribute_name = init_attribute\n",
        "    self.important_input = some_function(init_input)\n",
        "    # any other initialization steps\n",
        "\n",
        "  #some other method appropriate for the class\n",
        "  def some_other_method(self, input_as_appropriate):\n",
        "    pipe_output = another_function(input_as_appropriate, self.important_input)\n",
        "    return pipe_output\n",
        "```"
      ],
      "metadata": {
        "id": "vNqM5msuGdCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Breakout Room: Understanding your Classes (10 mins)\n",
        "In your breakout rooms, use your genAI of choice to better understand object-oriented design in the context of the implementation of these classes. Use the answers from the questions above to interrogate how this functionality is implemented in code.\n",
        "\n",
        "#### **Code**\n",
        "\n",
        "```\n",
        "# Import the libraries\n",
        "import openai\n",
        "import transformers\n",
        "\n",
        "# Define the AIModel base class\n",
        "class AIModel:\n",
        "    # Initialize the model name\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    # Define the get_answer method as an abstract method\n",
        "    def get_chat(self, question):\n",
        "        # Raise a NotImplementedError exception\n",
        "        raise NotImplementedError(\"The get_answer method must be implemented by the subclass.\")\n",
        "\n",
        "# Define the HFModel class as a subclass of AIModel\n",
        "class HFModel(AIModel):\n",
        "    # Initialize the model\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__(model_name)\n",
        "        self.model = transformers.pipeline(\"text-generation\", model=model_name)\n",
        "\n",
        "    # Define the get_answer method\n",
        "    def get_chat(self, question):\n",
        "        # Generate text using the model\n",
        "        output = self.model(question)\n",
        "        # Return the answer\n",
        "        answer = output[0][\"generated_text\"]\n",
        "        return answer\n",
        "\n",
        "# Define the OpenAIModel class as a subclass of AIModel\n",
        "class OpenAIModel(AIModel):\n",
        "    # Initialize the model and set the API key\n",
        "    def __init__(self, model_name, api_key):\n",
        "        super().__init__(model_name)\n",
        "        self.model = model_name\n",
        "        self.api_key = api_key\n",
        "        openai.api_key = api_key\n",
        "\n",
        "    # Define the get_answer method\n",
        "    def get_chat(self, question):\n",
        "        # Generate text using the OpenAI API\n",
        "        response = openai.Completion.create(\n",
        "            engine=self.model,\n",
        "            prompt=question,\n",
        "            max_tokens=100,\n",
        "            temperature=0.9,\n",
        "            stop=\"\\n\"\n",
        "        )\n",
        "        # Return the answer\n",
        "        answer = response[\"choices\"][0][\"text\"]\n",
        "        return answer\n",
        "```\n",
        "\n",
        "#### **Questions to Answer**\n",
        "\n",
        "_Note: here, you will need to design your questions to GenAI based on the answers to the questions above. Some example prompts are given, but first try to think of your own prompt to answer these questions. Make sure to add these (your initial prompts) to the breakout room document as well as your successful prompts._ \n",
        "\n",
        "**0. Explain at a high level the purpose of this code and general organization.**\n",
        "<details>\n",
        "<summary>Example prompts</summary>\n",
        "<blockquote>\n",
        "This code seems to implement `get_chat` functionality which could be implemented as a function. Why might someone instead implement the code like this? It seems more complex.\n",
        "</blockquote>\n",
        "<blockquote>\n",
        "Explain to me, in the simplest terms possible, each of the 3 classes. How do I use them?\n",
        "</blockquote>\n",
        "</details>\n",
        "\n",
        "**1. What functionality does each model type need to perform?**\n",
        "<details>\n",
        "<summary>Example prompts</summary>\n",
        "<blockquote>\n",
        "Do each of the classes all perform the same general functions?\n",
        "</blockquote>\n",
        "</details>\n",
        "\n",
        "**2. Does the functionality differ for `get_chat` for the different models?**\n",
        "<details>\n",
        "<summary>Example prompts</summary>\n",
        "<blockquote>\n",
        "If all of the classes perform the same general functions, why are there 3 definitions of `get_chat` when they all have the same underlying intent?\n",
        "</blockquote>\n",
        "</details>\n",
        "\n",
        "**3. What functionality is best left performed one time?**\n",
        "<details>\n",
        "<summary>Example prompts</summary>\n",
        "<blockquote>\n",
        "What is the purpose of the `__init__` function, and how do I use it to create an object?\n",
        "</blockquote>\n",
        "<blockquote>\n",
        "Why is the code for instantiating the model in the `__init__` function? I could reduce the number of methods in each class to just the `get_chat` method if I did. Are there advantages to this approach?\n",
        "</blockquote>\n",
        "</details>\n",
        "\n",
        "**4. What characteristics should be stored about each model?**\n",
        "<details>\n",
        "<summary>Example prompts</summary>\n",
        "<blockquote>\n",
        "What is the purpose of `self` in this code?\n",
        "</blockquote>\n",
        "<blockquote>\n",
        "It looks like sometimes, we use method inputs and do something like \"self.api_key = api_key\". Other times, we have lines like \"answer = output[0]['generated_text'] where we don't use the self part. Why is this?\n",
        "</blockquote>\n",
        "\n",
        "</details>\n",
        "\n",
        "**5. Speaking generally, what is common about these models?**\n",
        "<details>\n",
        "<summary>Example prompts</summary>\n",
        "<blockquote>\n",
        "What is the AIModel class? Can I use it directly?\n",
        "</blockquote>\n",
        "<blockquote>\n",
        "How is common functionality across both the HFModel and OpenAI captured in this code?\n",
        "</blockquote>\n",
        "\n",
        "</details>\n",
        "\n",
        "**6. (If time and context left) Get a summary of the conversation.**\n",
        "<details>\n",
        "<summary>Example prompts</summary>\n",
        "<blockquote>\n",
        "I am a beginner in learning about object oriented programming. Can you summarize our conversation in the clearest, most concise terms, highlighting the most relevant parts for my knowledge?\n",
        "</blockquote>\n",
        "</details>\n",
        "<br>\n",
        "\n",
        "#### **Code Generation Details**\n",
        "<details>\n",
        "<summary>Click here if interested in how generative AI was used to generate this code.</summary>\n",
        "\n",
        "The following prompts were used iteratively to reach a reasonable answer with BingGPT. Note that the part about the base class can be ignored, unless you simply want to implement a base class:\n",
        "<blockquote>\n",
        "\"Write me some python code which implements two classes: HFModel and OpenAIModel. They should leverage their own APIs and use the simplest, most straightforward implementation possible. The purpose of the two classes is to create text generation models, and have a get_answer method which utilizes their APIs to return the answer when called on the object.\"\n",
        "</blockquote>\n",
        "\n",
        "<blockquote>\n",
        "Can you add a base class called AIModel from which OpenAIModel and HFModel are derived? all models need to have a name, but only some models need an API key.\n",
        "</blockquote>\n",
        "\n",
        "<blockquote>\n",
        "Given that the HFModel uses a pipeline, is there any unnecessary code in the HFModel class definition, and can it be removed?\n",
        "</blockquote>\n",
        "\n",
        "<blockquote>\n",
        "Should the base model have a get_answer implementation? Is it better formed as \"pass\" or raising a NotImplementedexception? This method is essential for the child classes.\n",
        "</blockquote>\n",
        "\n",
        "<blockquote>\n",
        "For the HF model, it seems as if the \"name\" and the \"model_name\" are actually the same parameter. Would mind updating this so that \"model_name\" is used?\n",
        "</blockquote>\n",
        "\n",
        "<blockquote>\n",
        "Would you mind providing me the full code for all 3 classes?\n",
        "</blockquote>\n",
        "\n",
        "<blockquote>\n",
        "For the openAI model, it looks like \"name\" should actually be the parameter which sets the self.model parameter. Also, it seems as if it would be better named as \"model_name\". Would you mind updating the full code with these changes?\n",
        "</blockquote>\n",
        "</details>"
      ],
      "metadata": {
        "id": "s5l9_0MCIC8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using class definitions\n",
        "We've merely defined these classes - how exactly do we use them?"
      ],
      "metadata": {
        "id": "uGOAvsPY3Ctn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Make sure to load the relevant libraries\n",
        "%%capture\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "3I15jqXTLMZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries\n",
        "import openai\n",
        "import transformers\n",
        "\n",
        "# Define the AIModel base class\n",
        "class AIModel:\n",
        "    # Initialize the model name\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    # Define the get_answer method as an abstract method\n",
        "    def get_chat(self, question):\n",
        "        # Raise a NotImplementedError exception\n",
        "        raise NotImplementedError(\"The get_answer method must be implemented by the subclass.\")\n",
        "\n",
        "# Define the HFModel class as a subclass of AIModel\n",
        "class HFModel(AIModel):\n",
        "    # Initialize the model\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__(model_name)\n",
        "        self.model = transformers.pipeline(\"text-generation\", model=model_name)\n",
        "\n",
        "    # Define the get_answer method\n",
        "    def get_chat(self, question):\n",
        "        # Generate text using the model\n",
        "        output = self.model(question)\n",
        "        # Return the answer\n",
        "        answer = output[0][\"generated_text\"]\n",
        "        return answer\n",
        "\n",
        "# Define the OpenAIModel class as a subclass of AIModel\n",
        "class OpenAIModel(AIModel):\n",
        "    # Initialize the model and set the API key\n",
        "    def __init__(self, model_name, api_key):\n",
        "        super().__init__(model_name)\n",
        "        self.model = model_name\n",
        "        self.api_key = api_key\n",
        "        openai.api_key = api_key\n",
        "\n",
        "    # Define the get_answer method\n",
        "    def get_chat(self, question):\n",
        "        # Generate text using the OpenAI API\n",
        "        response = openai.Completion.create(\n",
        "            engine=self.model,\n",
        "            prompt=question,\n",
        "            max_tokens=100,\n",
        "            temperature=0.9,\n",
        "            stop=\"\\n\"\n",
        "        )\n",
        "        # Return the answer\n",
        "        answer = response[\"choices\"][0][\"text\"]\n",
        "        return answer"
      ],
      "metadata": {
        "id": "7NqlCilCLD1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## usage\n"
      ],
      "metadata": {
        "id": "L3huW4Cb2ZWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make your own package!\n",
        "We could actually even bundle this up into our own package for use. Note that there's some linter issues here, but otherwise, this should be functional. Note that the code used here is ever so slightly different."
      ],
      "metadata": {
        "id": "2qwqjTZ3TwcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Otherwise, pay no attention to the man behind the curtain...\n",
        "\n",
        "#...or do, I guess.\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create the top-level package directory\n",
        "os.makedirs('our_langchain', exist_ok=True)\n",
        "\n",
        "# Create the subpackage directory\n",
        "os.makedirs('our_langchain/llms', exist_ok=True)\n",
        "\n",
        "#Create the init files\n",
        "with open('our_langchain/__init__.py', 'w') as f:\n",
        "    f.write(\"__all__ = ['llms']\")\n",
        "\n",
        "with open('our_langchain/llms/__init__.py', 'w') as f:\n",
        "    f.write(\"from .llm_classes import OpenAIModel, HFModel\\n__all__ = ['OpenAIModel', 'HFModel']\")"
      ],
      "metadata": {
        "id": "qtBTm4_RULWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile our_langchain/llms/llm_classes.py\n",
        "# Import the libraries\n",
        "try:\n",
        "    import openai\n",
        "    import transformers\n",
        "except ImportError:\n",
        "    raise ImportError(\"You need to install the 'openai' and 'transformers' libraries. Please run 'pip install openai transformers'.\")\n",
        "import warnings\n",
        "\n",
        "\n",
        "# Define the AIModel base class\n",
        "class AIModel:\n",
        "    # Initialize the model name\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    # Define the get_answer method as an abstract method\n",
        "    def __call__(self, question):\n",
        "        # Raise a NotImplementedError exception\n",
        "        raise NotImplementedError(\"The get_answer method must be implemented by the subclass.\")\n",
        "\n",
        "# Define the HFModel class as a subclass of AIModel\n",
        "class HFModel(AIModel):\n",
        "    # Initialize the model\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__(model_name)\n",
        "\n",
        "        config_kwargs = {'pad_token_id':50256} if model_name=='gpt2' else {}\n",
        "        \n",
        "        with warnings.catch_warnings():\n",
        "          warnings.simplefilter(\"ignore\")\n",
        "          self.model = transformers.pipeline(\"text-generation\", model=model_name, model_kwargs=config_kwargs)\n",
        "\n",
        "    # Define the get_answer method\n",
        "    def __call__(self, question):\n",
        "        # Generate text using the model\n",
        "        with warnings.catch_warnings():\n",
        "          warnings.simplefilter(\"ignore\")\n",
        "          output = self.model(question)\n",
        "        # Return the answer\n",
        "        answer = output[0][\"generated_text\"]\n",
        "        return answer\n",
        "\n",
        "# Define the OpenAIModel class as a subclass of AIModel\n",
        "class OpenAIModel(AIModel):\n",
        "    # Initialize the model and set the API key\n",
        "    def __init__(self, model_name, api_key):\n",
        "        super().__init__(model_name)\n",
        "        self.model = model_name\n",
        "        self.api_key = api_key\n",
        "        openai.api_key = api_key\n",
        "\n",
        "    # Define the get_answer method\n",
        "    def __call__(self, question):\n",
        "        # Generate text using the OpenAI API\n",
        "        response = openai.Completion.create(\n",
        "            engine=self.model,\n",
        "            prompt=question,\n",
        "            max_tokens=100,\n",
        "            temperature=0.9,\n",
        "            stop=\"\\n\"\n",
        "        )\n",
        "        # Return the answer\n",
        "        answer = response[\"choices\"][0][\"text\"]\n",
        "        return answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2OWy1U5UEKo",
        "outputId": "4a1a6db8-2b65-411b-f44b-2a31fe9d91aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting our_langchain/llms/llm_classes.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Excitingly, based on the code we have just written for the classes, this magical cell has created your very own package!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vR5J7WFUczc",
        "outputId": "234bb776-3055-4dfa-b3d6-041f13cdd51b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excitingly, based on the code we have just written for the classes, this magical cell has created your very own package!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the package"
      ],
      "metadata": {
        "id": "zoGLY4d-4B33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import our module\n"
      ],
      "metadata": {
        "id": "LpDNWrQpMgWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a HF model with some text input\n"
      ],
      "metadata": {
        "id": "NiURXdZtYGRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make another call\n"
      ],
      "metadata": {
        "id": "AWEl_bybYmXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# APIs"
      ],
      "metadata": {
        "id": "lyjc6m0xTajL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Packages, Libraries, and Modules\n",
        "**Was the function implementation or the class implementation easier? Which one do you think is more useful from a library point of view? Why?**\n",
        "Let's look at [langchain](https://github.com/hwchase17/langchain/tree/master/langchain/llms).\n",
        "\n",
        "This leads to the practical structuring of tons of functions and classes. These tend to take shape through **packages**, **libraries**, **modules**, and **classes**.\n",
        "\n",
        "<center>\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Package  Hierarchy  Model</th>\n",
        "    <th>Kitchen Package Application</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th><img style=\"vertical-align: bottom;\" src=\"https://github.com/vanderbilt-data-science/p4ai-essentials/blob/main/img/class_package_hierarchy.png?raw=true\" width=100% /></th>\n",
        "    <th><img style=\"vertical-align: bottom;\" src=\"https://github.com/vanderbilt-data-science/p4ai-essentials/blob/main/img/kitchen_package_hierarchy.png?raw=true\" width=100% /></th>\n",
        "  </tr>\n",
        "</table>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "TroHymrdxReH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examples of Packages, Libraries, and Modules\n",
        "We've already been leveraging this functionality, actually, through:\n",
        "\n",
        "* Using the OpenAI Python-bindings API (`import openai`)\n",
        "* Importing something specific from a library (`from transformers import pipeline`)\n",
        "\n",
        "* Langchain:\n",
        "\n",
        "<center>\n",
        "<figure>\n",
        "<img src=\"https://pbs.twimg.com/media/FvK0MmTaAAIPkUI?format=jpg&name=large\" width=\"800\"/>\n",
        "<figcaption>Image from: Twitter user<a href=\"https://twitter.com/pwang_szn\"> pwang_szn</a></figcaption>\n",
        "</figure>\n",
        "</center>"
      ],
      "metadata": {
        "id": "MYg3gSqf0ZL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## APIs\n",
        "\n",
        "APIs define the way in which you can programmatically interact with a codebase, server, etc. It's essentially a contract outlining:\n",
        "* The available tasks that can be done by the framework and the names these operations are called by\n",
        "* The required and optional inputs to these operations\n",
        "* The required and optional outputs from these operations.\n",
        "\n",
        "APIs often come in the form of **libraries**, **packages**, and **modules**. Libraries are a great way of quickly infusing Python with LOTS more functionality.\n",
        "\n",
        "<center>\n",
        "<figure>\n",
        "<img src=\"https://cdn.mos.cms.futurecdn.net/GSkcxRqtHam58T5URwTN7c-1024-80.jpg.webp\" width=\"300\"/>\n",
        "<figcaption>Image from livescience.com</figcaption>\n",
        "</figure>\n",
        "</center>\n",
        "\n",
        "I always think of importing modules similarly to this scene in The Matrix. Information \"packages\" on how to fly a helicopter or kung fu are downloaded instantly into the user's \"kernel\". Then, we need to use the functions from these packages.\n",
        "\n",
        "Let's explore some APIs and see if what we've done today looks familiar.\n",
        "* [Langchain Documentation](https://python.langchain.com/en/latest/index.html)\n",
        "* [HuggingFace Documentation](https://huggingface.co/docs/transformers/index)\n",
        "* [OpenAI Documentation](https://platform.openai.com/docs/api-reference)"
      ],
      "metadata": {
        "id": "H8Hkqoq5fEpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Breakout Room (10 minutes)\n",
        "Select one of the 3 APIs from above. Using one of the quickstart pages, tutorials, answer the following questions:\n",
        "1. What is the package being used?\n",
        "2. What is being imported? Does it appear to be imported from a module? Which module and how can you tell?\n",
        "3. Try to run the quickstart code to try out the example.\n",
        "4. Summarize what you think the code is doing. Otherwise, use a GenAI to help summarize the behavior."
      ],
      "metadata": {
        "id": "5SsaRk_Hexv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Congratulations!\n",
        "You made it through Day 2 of AI-Assisted Programming. We covered:\n",
        "- Functions\n",
        "- Object-oriented programming concepts\n",
        "  - Classes\n",
        "  - Packages, even made our own package!\n",
        "- APIs and what they mean\n",
        "\n",
        "In our next session, we'll cover more to help you program effectively with GenAI!"
      ],
      "metadata": {
        "id": "AFRZKuX16ocG"
      }
    }
  ]
}