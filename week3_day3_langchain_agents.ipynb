{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUsYtbmBnPwl/VxN3C4jJF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanderbilt-data-science/ai_summer/blob/main/week3_day3_langchain_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Summer Week 3, Day 3\n",
        "# Langchain Agents\n",
        "\n",
        "[Agents](https://python.langchain.com/en/latest/modules/agents.html) are a powerful way to allow a LLM to make decisions and access tools in an automated fashion. Agents are ideal when a model needs to access the internet, perform mathematical operations, or access an API or other outside resource to respond to a prompt."
      ],
      "metadata": {
        "id": "HuoUex0fmha3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "JxcOfi7Amah3"
      },
      "outputs": [],
      "source": [
        "# Download required packages to Colab\n",
        "!pip install -q langchain\n",
        "!pip install -q openai\n",
        "!pip install -q google-search-results\n",
        "!pip install -q chromadb\n",
        "!pip install -q tiktoken # needed for chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import load_tools, initialize_agent, AgentType\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain import OpenAI, VectorDBQA\n",
        "import os\n",
        "from getpass import getpass"
      ],
      "metadata": {
        "id": "JGtCQSUAng-M"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = getpass()\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyKMq1QvnqWi",
        "outputId": "1f0e4d81-2208-4ed7-d759-ffa9dc51cc60"
      },
      "execution_count": 47,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take 5 minutes to sign up for a SERP API key: https://serper.dev/"
      ],
      "metadata": {
        "id": "gOgnIfNUFmyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "serp_api_key = getpass()\n",
        "os.environ[\"SERPAPI_API_KEY\"] = serp_api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3Brf0GKC4I9",
        "outputId": "cc5a7b29-7628-4eb2-c9a8-a33bb8ace258"
      },
      "execution_count": 48,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Types\n",
        "\n",
        "Langchain supports four basic Agent Types, accessing using the AgentType class:\n",
        "\n",
        "1. ZERO_SHOT_REACT_DESCRIPTION: a basic Agent that chooses tools based on their descriptions. Requires tools, a model, and an input string\n",
        "2. CONVERSATIONAL_REACT_DESCRIPTION: an Agent that uses memory to keep track of chat history. Can answer questions using tools or previous chats. Requires tools, a model, and a dictionary containing chat history and an input string\n",
        "3. SELF_ASK_WITH_SEARCH: an Agent that uses a single tool to find factual answers. Requires one tool named \"Intermediate Answer\", a model, and input.\n",
        "4. REACT_DOCSTORE: an Agent that uses the [ReAct framework](https://arxiv.org/pdf/2210.03629.pdf) to interact with a document store. Requires a \"Search\" tool and a \"Lookup\" tool (must have those exact names, a model, and input. The \"Search\" tool must search for a document, and the \"Lookup\" tool must look up a term in the found document.\n",
        "\n",
        "The easiest to use is the ZERO_SHOT_REACT_DESCRIPTION, since it does not require any specific tools. Simply initialize the agent with the loaded tools (from `load_tools`), model, and `AgentType.ZERO_SHOT_REACT_DESCRIPTION` in the agent parameter."
      ],
      "metadata": {
        "id": "XFs9ZNbGU26B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the LLM\n",
        "llm = OpenAI(temperature = 0.5)"
      ],
      "metadata": {
        "id": "YkgO5m_2p6RY"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Language model already loaded in previous cells\n",
        "# Now we add tools to the llm\n",
        "\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm = llm)"
      ],
      "metadata": {
        "id": "4ssiuj9cn0sA"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize agent with tools, llm, and agent type\n",
        "agent = initialize_agent(tools, llm, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True)"
      ],
      "metadata": {
        "id": "WFDTDEk1n2jU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"What was the high temperature in Nashville yesterday in Fahrenheit? What is that number raised to the 0.23 power?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "pZf2TSRIn4Tj",
        "outputId": "64ab311d-c1de-4151-eb9d-1078c8b97379"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m Temperature is a current event, so I should search for it.\n",
            "Action: Search\n",
            "Action Input: \"high temperature Nashville yesterday Fahrenheit\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mHigh: 69.8ºf @12:00 AM Low: 59ºf @6:15 AM Approx. Precipitation / Rain Total: in.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use the calculator to raise the temperature to the 0.23 power\n",
            "Action: Calculator\n",
            "Action Input: 69.8^0.23\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.6551358533283302\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: 2.6551358533283302\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6551358533283302'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Breakout Exercise (10 mins)\n",
        "\n",
        "Using the agent we created, come up with a new question to ask the model. What decisions did the agent make? Did it arrive at the right answer? Be prepared to share interesting results!\n",
        "\n",
        "If you have time, play around with the tools you give the agent by adding new tools or taking away previous tools. How do its answers change? "
      ],
      "metadata": {
        "id": "SMcmsUADSSxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversation Agent"
      ],
      "metadata": {
        "id": "F7B9qzaMTfSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Conversation Agent uses memory to act more like a chat bot by taking message history into account. To set up a Conversation Agent, you first need to set up its memory. For this example, we will use the ConversationBufferMemory and give it the name \"chat_history\". "
      ],
      "metadata": {
        "id": "bRcvagr6WGQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
      ],
      "metadata": {
        "id": "S7OAL5S2V_Og"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can pass our memory into the memory parameter for the `initialize_agent` function. "
      ],
      "metadata": {
        "id": "rTWu4-PnWUMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tools, llm, agent = AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose = True, memory = memory)\n"
      ],
      "metadata": {
        "id": "0ClPJi7cS_ol"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try out our bot."
      ],
      "metadata": {
        "id": "-NEChanQWc49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(input =\"What was the high temperature in Nashville yesterday in Fahrenheit?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "XVTMEvoRV2GJ",
        "outputId": "3bbce783-15f9-4bf0-a4bb-4610ae1eb89a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Thought: Do I need to use a tool? Yes\n",
            "Action: Search\n",
            "Action Input: High temperature in Nashville yesterday in Fahrenheit\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mHigh: 69.8ºf @12:05 AM Low: 59ºf @5:10 AM Approx. Precipitation / Rain Total: in.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
            "AI: The high temperature in Nashville yesterday in Fahrenheit was 69.8ºf.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The high temperature in Nashville yesterday in Fahrenheit was 69.8ºf.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(input = \"What is that number raised to the 0.23 power?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "_dWvAyIvV4KV",
        "outputId": "67eee4e0-84cf-41f6-bb4c-df8f4565f8a8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
            "Action: Calculator\n",
            "Action Input: 69.8^0.23\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.6551358533283302\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
            "AI: The answer is 2.6551358533283302.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The answer is 2.6551358533283302.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that this Agent first asks itself if it needs a tool to answer your query. "
      ],
      "metadata": {
        "id": "rNS_MVE2WqFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Breakout Exercise (10 mins)\n",
        "Play around with the ConversationAgent. How does its behavior differ from the ZeroShotReactDescription Agent?"
      ],
      "metadata": {
        "id": "YEeKggU3cfzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VectorStore Agent"
      ],
      "metadata": {
        "id": "vMsl5_6gSD1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create our example VectorStores. The first VectorStore is the state of the union text file, which we grab using `wget`. The next VectorStore is the FAQ documentation for the RUFF library, which we grab from the web and load using Langchain's WebBaseLoader."
      ],
      "metadata": {
        "id": "XuYEnwgiYM4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ3QKk7aXuYF",
        "outputId": "1e8e8bf5-ed0a-40e0-cb68-98f39011d668"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-25 19:43:19--  https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39027 (38K) [text/plain]\n",
            "Saving to: ‘state_of_the_union.txt’\n",
            "\n",
            "state_of_the_union. 100%[===================>]  38.11K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2023-05-25 19:43:19 (12.4 MB/s) - ‘state_of_the_union.txt’ saved [39027/39027]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# Assign Loader to document\n",
        "loader = TextLoader('./state_of_the_union.txt')\n",
        "\n",
        "# Load documents\n",
        "documents = loader.load()\n",
        "\n",
        "# Create text splitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "\n",
        "# Split texts\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Create embeddings \n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Apply embeddings to texts, and upload those to a Chroma database\n",
        "state_of_union_store = Chroma.from_documents(texts, embeddings, collection_name=\"state-of-union\") # Note we can assign it a collection name "
      ],
      "metadata": {
        "id": "K9IP_xJ3XrCb"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "noacz6rsXsDD"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agents only interact with tools and input. To allow your Agent to access a database, you must initialize the database as a tool. We can accomplish this by using Langchain's VectorStoreToolkit, a class that creates a tool from a VectorStore, given the VectorStore and it's metadata (name, description, etc.)."
      ],
      "metadata": {
        "id": "WXdXp00PZTxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.agent_toolkits import (\n",
        "    create_vectorstore_agent,\n",
        "    VectorStoreToolkit,\n",
        "    VectorStoreInfo,\n",
        ")\n",
        "\n",
        "# Create the VectorStoreInfo (so our Agent understands what information our VectorStore holds)\n",
        "vectorstore_info = VectorStoreInfo(\n",
        "    name=\"state_of_union_address\",\n",
        "    description=\"the most recent state of the Union adress\",\n",
        "    vectorstore=state_of_union_store\n",
        ")\n",
        "\n",
        "# Give the VectorStoreInfo to our VectorStoreToolkit to create the tool\n",
        "toolkit = VectorStoreToolkit(vectorstore_info=vectorstore_info)\n",
        "\n",
        "# Use create_vectorstore_agent to initalize an Agent specifically for searching databases\n",
        "agent_executor = create_vectorstore_agent(\n",
        "    llm=llm,\n",
        "    toolkit=toolkit,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "0e4029LFZeEq"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.run(\"What did biden say about ketanji brown jackson is the state of the union address?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "ZlzB3TrfbXC0",
        "outputId": "961d9145-6640-4a64-f2ee-648ac169f348"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find the answer in the state of the union address\n",
            "Action: state_of_union_address\n",
            "Action Input: What did biden say about ketanji brown jackson\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m Biden said that Ketanji Brown Jackson is one of the nation's top legal minds and that she will continue Justice Breyer's legacy of excellence.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Biden said that Ketanji Brown Jackson is one of the nation's top legal minds and that she will continue Justice Breyer's legacy of excellence.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Biden said that Ketanji Brown Jackson is one of the nation's top legal minds and that she will continue Justice Breyer's legacy of excellence.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Multiple VectorStores\n",
        "\n",
        "Sometimes your information may be spread across multiple VectorStores. In that case, your Agent will need to decide when to use which VectorStore. This is called \"routing,\" and requires the VectorStoreRouterToolkit and the `create_vectorstore_router_agent`."
      ],
      "metadata": {
        "id": "ub2c7cFgaMYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "# Assign loader to document (in this case, a website)\n",
        "loader = WebBaseLoader(\"https://beta.ruff.rs/docs/faq/\")\n",
        "\n",
        "# Load documents\n",
        "docs = loader.load()\n",
        "\n",
        "# Split text (using text splitter from previous cell)\n",
        "ruff_texts = text_splitter.split_documents(docs)\n",
        "\n",
        "# Apply embeddings (using embeddings from previous cell) and upload to Chroma database\n",
        "ruff_store = Chroma.from_documents(ruff_texts, embeddings, collection_name=\"ruff\")  # Note we can assign it a collection name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WilUuaw5aWe3",
        "outputId": "8b3e70c7-235d-4b24-9511-912ed6110185"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 1356, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1923, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1267, which is longer than the specified 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.agent_toolkits import (\n",
        "    create_vectorstore_router_agent,\n",
        "    VectorStoreRouterToolkit,\n",
        "    VectorStoreInfo,\n",
        ")"
      ],
      "metadata": {
        "id": "yCS2qL0AaaA0"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create VectorStoreInfo (note we already created vectorstore_info for state of the union address)\n",
        "\n",
        "ruff_vectorstore_info = VectorStoreInfo(\n",
        "    name=\"ruff\",\n",
        "    description=\"Information about the Ruff python linting library\",\n",
        "    vectorstore=ruff_store\n",
        ")\n",
        "\n",
        "# Give ruff_vectorstore_info and vectorstore_info to VectorStoreRouterToolkit\n",
        "router_toolkit = VectorStoreRouterToolkit(\n",
        "    vectorstores=[vectorstore_info, ruff_vectorstore_info],\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "# Use create_vectorstore_router_agent to inialize an Agent for routing between databases\n",
        "agent_executor = create_vectorstore_router_agent(\n",
        "    llm=llm,\n",
        "    toolkit=router_toolkit,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "G2UWYwima0LW"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.run(\"What did biden say about ketanji brown jackson is the state of the union address?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "9rAccsFebZFE",
        "outputId": "73a0de1e-6834-4b42-bc05-34698d06084d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use the state of the union address tool to answer this question.\n",
            "Action: state_of_union_address\n",
            "Action Input: What did biden say about ketanji brown jackson\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m Biden said that Ketanji Brown Jackson is one of the nation's top legal minds and that she will continue Justice Breyer's legacy of excellence.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Biden said that Ketanji Brown Jackson is one of the nation's top legal minds and that she will continue Justice Breyer's legacy of excellence.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Biden said that Ketanji Brown Jackson is one of the nation's top legal minds and that she will continue Justice Breyer's legacy of excellence.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.run(\"What tool does ruff use to run over Jupyter Notebooks?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "7W7IRg8abZ7c",
        "outputId": "8876d49c-387d-43b0-9f5f-0ce6efa28e4b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should look for a tool that is specific to ruff\n",
            "Action: ruff\n",
            "Action Input: What tool does ruff use to run over Jupyter Notebooks?\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m Ruff is integrated into nbQA, a tool for running linters and code formatters over Jupyter Notebooks.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Ruff is integrated into nbQA, a tool for running linters and code formatters over Jupyter Notebooks.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ruff is integrated into nbQA, a tool for running linters and code formatters over Jupyter Notebooks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.run(\"What tool does ruff use to run over Jupyter Notebooks? Did the president mention that tool in the state of the union?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "s1FtRS7_beBG",
        "outputId": "945d1fab-5eda-42a4-d3fc-6f6440f048ff"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m Ruff is a python linting library, and the state of the union address is a speech.\n",
            "Action: ruff\n",
            "Action Input: What tool does ruff use to run over Jupyter Notebooks?\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m Ruff is integrated into nbQA, a tool for running linters and code formatters over Jupyter Notebooks.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m The state of the union address is a speech, so it is unlikely that the president would mention a specific tool.\n",
            "Final Answer: Ruff is integrated into nbQA, a tool for running linters and code formatters over Jupyter Notebooks. It is unlikely that the president mentioned this tool in the state of the union address.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ruff is integrated into nbQA, a tool for running linters and code formatters over Jupyter Notebooks. It is unlikely that the president mentioned this tool in the state of the union address.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class Discussion\n",
        "What would you use Agents for (personally or for work/research/projects)? \n",
        "\n"
      ],
      "metadata": {
        "id": "9JsNOrghcuk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8ZTK4xIqdDVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What would you **not** use Agents for?"
      ],
      "metadata": {
        "id": "ONVWmZ8QdERt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r0YE-QCzdGMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Do any interesting applications come to mind?"
      ],
      "metadata": {
        "id": "Lz9t-iPWdGha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qq5iBO2ydHJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problems with Agents"
      ],
      "metadata": {
        "id": "FK-gF2dVcNFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are some problems you have noticed with Agents? Could these be solved somehow?\n",
        "\n",
        "Something to consider - https://github.com/hwchase17/langchain/issues/4102"
      ],
      "metadata": {
        "id": "8DV5UiY6ncx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AbZ-dIHTdJau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework\n",
        "\n",
        "Prepare a solution to your own problem using Agents. Assess how the Agent did. What type of Agent did you use? \n",
        "\n",
        "Play around with the temperature of the model. How does that affect the results?"
      ],
      "metadata": {
        "id": "smhGuJ0JFJEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE #"
      ],
      "metadata": {
        "id": "_a7FY-4-dKwX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}