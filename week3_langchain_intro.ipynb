{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanderbilt-data-science/ai_summer/blob/main/week3_langchain_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a8ac011",
      "metadata": {
        "id": "2a8ac011"
      },
      "source": [
        "# Intro to Langchain\n",
        "\n",
        "These examples are mostly from LangChain's [QuickStart Guide](https://python.langchain.com/en/latest/getting_started/getting_started.html)\n",
        "\n",
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are running this in Google Colab, you will need to install the libraries using the cell below:"
      ],
      "metadata": {
        "id": "gxLzCsHDFi8j"
      },
      "id": "gxLzCsHDFi8j"
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required packages to Colab\n",
        "!pip install -q langchain\n",
        "!pip install -q openai\n",
        "!pip install -q chromadb\n",
        "!pip install -q tiktoken\n",
        "!pip install -q duckduckgo-search"
      ],
      "metadata": {
        "id": "92eUGxpfFq3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254ab600-1202-4a3d-cf0c-d68633c56bc2"
      },
      "id": "92eUGxpfFq3F",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m858.2/858.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.3/71.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.6/922.6 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m757.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.1/414.1 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.30.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "76f9676c",
      "metadata": {
        "id": "76f9676c"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain, RetrievalQA\n",
        "from langchain import ConversationChain\n",
        "from langchain.agents import load_tools, initialize_agent, AgentType\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from langchain.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
        "import os\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI API key\n",
        "Get an OpenAI [API token](https://platform.openai.com/docs/api-reference) and save it somewhere secure. The below cell uses the package `getpass` to allow you to enter the API key securely (like you would a password).\n",
        "\n",
        "*Note: this cell will not finish running until you enter the API key*"
      ],
      "metadata": {
        "id": "WgGplInnGFFT"
      },
      "id": "WgGplInnGFFT"
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = getpass()\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n"
      ],
      "metadata": {
        "id": "M-myC8PUHdLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac6ab433-db55-458f-d152-270a0fbd4a57"
      },
      "id": "M-myC8PUHdLS",
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7221ad84",
      "metadata": {
        "id": "7221ad84"
      },
      "source": [
        "## Get predictions from language model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose your model\n",
        "LangChain has a list of available [models](https://python.langchain.com/en/latest/modules/models.html). Choose the one that is best for your application. In this case, we are using OpenAI because we have the API key for it."
      ],
      "metadata": {
        "id": "SLaRBtgKI3vY"
      },
      "id": "SLaRBtgKI3vY"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6c424521",
      "metadata": {
        "id": "6c424521"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "71187944",
      "metadata": {
        "id": "71187944",
        "outputId": "f6896307-0d44-4381-ba36-178ed8f104a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Brightly Toes Socks\n"
          ]
        }
      ],
      "source": [
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "006bca95",
      "metadata": {
        "id": "006bca95"
      },
      "source": [
        "## Manage Prompts for LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain allows you to format prompts to better control user input using the [PromptTemplate](https://python.langchain.com/en/latest/modules/prompts/prompt_templates/getting_started.html) class."
      ],
      "metadata": {
        "id": "9BLm9Pv4JQuC"
      },
      "id": "9BLm9Pv4JQuC"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "574eacdc",
      "metadata": {
        "id": "574eacdc"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables = [\"product\"],\n",
        "    template = \"What is a good name for a company that makes {product}?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the `format` method to fill in a value for the parameter in brackets."
      ],
      "metadata": {
        "id": "pOO_uDuI0hb4"
      },
      "id": "pOO_uDuI0hb4"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a279ce87",
      "metadata": {
        "id": "a279ce87",
        "outputId": "1511c627-e5f0-4c7c-fe4e-ca7ba0fa3bf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is a good name for a company that makes modern clothing?\n"
          ]
        }
      ],
      "source": [
        "new_prompt = prompt.format(product=\"modern clothing\")\n",
        "print(new_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3a9d0c6a",
      "metadata": {
        "id": "3a9d0c6a",
        "outputId": "c70447ed-8363-4891-e128-f5447644a2be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Urban Boutique.\n"
          ]
        }
      ],
      "source": [
        "print(llm(new_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4548164",
      "metadata": {
        "id": "b4548164"
      },
      "source": [
        "## Using chains to create a workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Chains](https://python.langchain.com/en/latest/modules/chains/getting_started.html) allow you to combine models and prompts for efficient workflows. This allows you to experiment with different models and prompts, all while using the same chain workflow. The base chain is LLMChain."
      ],
      "metadata": {
        "id": "zszKItTjJv6P"
      },
      "id": "zszKItTjJv6P"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5af1e212",
      "metadata": {
        "id": "5af1e212"
      },
      "outputs": [],
      "source": [
        "chain = LLMChain(llm = llm, prompt = prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ede52d9e",
      "metadata": {
        "id": "ede52d9e",
        "outputId": "f4a6b8e0-0012-4abc-c6a8-97063d3e148e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nWackyWearz Socks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "chain.run(\"colorful socks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `chain.apply()` to apply a list of inputs to the model."
      ],
      "metadata": {
        "id": "h6dBZLdJo6p_"
      },
      "id": "h6dBZLdJo6p_"
    },
    {
      "cell_type": "code",
      "source": [
        "input_list = [\n",
        "    {\"product\": \"socks\"},\n",
        "    {\"product\": \"computer\"},\n",
        "    {\"product\": \"shoes\"}\n",
        "]\n",
        "\n",
        "chain.apply(input_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT-GX9yZo0O3",
        "outputId": "805b7421-c117-4e5c-d94d-6a6c7257c2b1"
      },
      "id": "yT-GX9yZo0O3",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': '\\n\\nSockItAway.'},\n",
              " {'text': '\\n\\nTechsters Inc.'},\n",
              " {'text': '\\n\\nFootFashions.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Breakout Exercise (10 mins)\n",
        "Change the Prompt Template to reflect a different question. Use the LLMChain to run a prediction. \n",
        "\n",
        "Next, if you have time, create a list of inputs and use the `apply` method to apply a list of inputs to your model. \n",
        "\n",
        "Some example questions if you are having trouble thinking of more:\n",
        "\n",
        "- \"What is a good title for a nonfiction book about {topic}?\"\n",
        "- \"Give me a recommendation for a {genre} movie.\"\n"
      ],
      "metadata": {
        "id": "QjK_w2AFnSe8"
      },
      "id": "QjK_w2AFnSe8"
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE "
      ],
      "metadata": {
        "id": "ifuvdzDDnR1g"
      },
      "id": "ifuvdzDDnR1g",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7807426f",
      "metadata": {
        "id": "7807426f"
      },
      "source": [
        "## Chain for Memory and State Changes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use special chains (such as the ConversationChain) to allow your model to remember past inputs. Note that each chain has its own set of methods to send input (in this case, `predict` instead of `run`)"
      ],
      "metadata": {
        "id": "JALS2Ub6Lqyl"
      },
      "id": "JALS2Ub6Lqyl"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b808ba48",
      "metadata": {
        "id": "b808ba48"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(llm = llm, verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a1c1ae80",
      "metadata": {
        "id": "a1c1ae80",
        "outputId": "86e40fc3-1b76-4906-999b-76c9b10905c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hi! How can I help you?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "conversation.predict(input = \"Hi there!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f8553905",
      "metadata": {
        "id": "f8553905",
        "outputId": "7ae47e93-2290-4c21-fd5a-cdc3819ad33c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi! How can I help you?\n",
            "Human: My name is Myranda.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Nice to meet you, Myranda! Is there anything I can do for you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "conversation.predict(input = \"My name is Myranda.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Types of Chains \n",
        "There are many chains supported on Langchain for many different tasks. We have just used the general LLMChain and ConversationChain. Read over the complete list of supported chains [here](https://python.langchain.com/en/latest/modules/chains/how_to_guides.html).\n",
        "\n",
        "If you can't find the chain you're looking for, try making your own [custom chain](https://python.langchain.com/en/latest/modules/chains/generic/custom_chain.html)!"
      ],
      "metadata": {
        "id": "vAfaCWf_mWn1"
      },
      "id": "vAfaCWf_mWn1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Breakout Exercise (10 mins)\n",
        "In your breakout rooms, look over the list of supported chains on Langchain and read their documentation. Do you find any of these chains particularly revelant to your project/interests? Share with your group and discuss what you could use these chains for! Or, describe how you would create a custom chain for your task.\n",
        "\n",
        "If you would like (and have time), try experimenting with different chains in the code cell below."
      ],
      "metadata": {
        "id": "BS881kudqdMo"
      },
      "id": "BS881kudqdMo"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9e2f02c4",
      "metadata": {
        "id": "9e2f02c4"
      },
      "outputs": [],
      "source": [
        "## Optional: Experiment with a chain in this cell\n",
        "## Note: some chains require additional inputs, such as a database"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb86f047",
      "metadata": {
        "id": "bb86f047"
      },
      "source": [
        "## Agents to Dynamically Call Chains"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agents allow the model to make decisions on the tools needed to respond to user input. The below example uses an agent in combination with the search engine DuckDuckGo API and the math tool llm-math. The model can then use these tools to output the proper response. "
      ],
      "metadata": {
        "id": "uFhdHVGWKPc3"
      },
      "id": "uFhdHVGWKPc3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tools and Agents"
      ],
      "metadata": {
        "id": "SagZL7VhKpFA"
      },
      "id": "SagZL7VhKpFA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose your tools (see available tools [here](https://python.langchain.com/en/latest/modules/agents/tools.html)). In this example we will use ddg-search (to access DuckDuckGo search engine) and llm-math (for mathematical operations). The load_tools function allows you to attach the tools to the LLM model."
      ],
      "metadata": {
        "id": "jp9ismcsK1YL"
      },
      "id": "jp9ismcsK1YL"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "0dbd6863",
      "metadata": {
        "id": "0dbd6863"
      },
      "outputs": [],
      "source": [
        "# Language model already loaded in previous cells\n",
        "# Now we add tools to the llm\n",
        "\n",
        "tools = load_tools([\"ddg-search\", \"llm-math\"], llm = llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we initialize our Agent by passing the `initialize_agent` function the tools, model, and agent type. You can read more about [agent types](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html) and their purposes, but for now we will use a general purpose one called ZERO_SHOT_REACT_DESCRIPTION. This Agent decides which tools to use based on the tools' built-in descriptions."
      ],
      "metadata": {
        "id": "gpVEu4r6ncO5"
      },
      "id": "gpVEu4r6ncO5"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "05c897ee",
      "metadata": {
        "id": "05c897ee"
      },
      "outputs": [],
      "source": [
        "# initialize agent with tools, llm, and agent type\n",
        "agent = initialize_agent(tools, llm, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "85f17084",
      "metadata": {
        "id": "85f17084",
        "outputId": "2a9ce733-fe63-442c-8609-70f9f5d277b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use DuckDuckGo Search to find out the temperature in Nashville.\n",
            "Action: DuckDuckGo Search\n",
            "Action Input: High temperature in Nashville yesterday in Fahrenheit\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mNashville Temperature Yesterday. Maximum temperature yesterday: 76 °F (at 12:53 pm) Minimum temperature yesterday: 67 °F (at 3:53 am) Average temperature yesterday: 71 °F. High & Low Weather Summary for the Past Weeks Temperature Humidity Pressure; High: 86 °F (May 7, 1:53 pm) 94% (May 7, 8:23 pm) Current Weather. 6:26 PM. 71° F. RealFeel® Sun 70°. RealFeel Shade™ 70°. Air Quality Fair. Wind NW 5 mph. Wind Gusts 11 mph. Mostly cloudy More Details. The high temperature in Nashville has hit the century mark for the first time in 10 years according to the National Weather Service. ... At 2:44 p.m., Nashville's temperature reached 101°. This ... Temperatures this morning are starting off in the mid to upper 40's which is 20 degrees warmer than yesterday! ... NASHVILLE, Tenn. (WKRN) - Temperatures cracked the 60s for highs in many ... Current weather in Nashville and forecast for today, tomorrow, and next 14 days\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I can use a calculator to find the 0.23 power of the temperature.\n",
            "Action: Calculator\n",
            "Action Input: 76^0.23\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.7076163393689234\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The temperature in Nashville yesterday raised to the 0.23 power is 2.7076163393689234.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The temperature in Nashville yesterday raised to the 0.23 power is 2.7076163393689234.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "agent.run(\"What was the high temperature in Nashville yesterday in Fahrenheit? What is that number raised to the 0.23 power?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXTRA Breakout Exercise (10 mins)\n",
        "Using the agent we created, come up with a new question to ask the model. What decisions did the agent make? Did it arrive at the right answer? Be prepared to share interesting results!\n",
        "\n",
        "If you have time, play around with the tools you give the agent by adding new tools or taking away previous tools. How do its answers change? "
      ],
      "metadata": {
        "id": "Cqlzs3P6rXWu"
      },
      "id": "Cqlzs3P6rXWu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexes"
      ],
      "metadata": {
        "id": "UoDdu1Npr9Zt"
      },
      "id": "UoDdu1Npr9Zt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Besides sending a prompt as input to a model, we can also give our model documents for analysis. However, sending an entire document as prompt quickly uses up the maximum context length. To solve this, we use indexes. \n",
        "\n",
        "[Indexes](https://python.langchain.com/en/latest/modules/indexes/getting_started.html) are a more memory-efficient way of storing documents that allow Langchain to look up relevant information from the document and only pass that snippet to the model. There are four main components of an Index: **Document Loader** (loading the document), **Text Splitter** (splitting text into smaller chunks), **Vector Store** (storing texts as vectors), and **Retriever** (retrieving the relevant text). There are several ways to implement this which we will further explore later. For now, let's look at an example.  "
      ],
      "metadata": {
        "id": "KmUVS8kk9kXW"
      },
      "id": "KmUVS8kk9kXW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Download example text file to Colab\n",
        "!wget https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omedJy8gsANh",
        "outputId": "a5898af5-1155-4ef7-9822-b4c7e2165338"
      },
      "id": "omedJy8gsANh",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-18 21:40:42--  https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39027 (38K) [text/plain]\n",
            "Saving to: ‘state_of_the_union.txt’\n",
            "\n",
            "state_of_the_union. 100%[===================>]  38.11K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2023-05-18 21:40:42 (10.8 MB/s) - ‘state_of_the_union.txt’ saved [39027/39027]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Documents\n",
        "Langchain has many types of Document Loaders, including ones for text files, PDFs, and Python files. In this example, we are working with a text file, so we will use the TextLoader. \n",
        "\n",
        "If you look at the cell toward the beginning of the notebook where we imported libraries, you will see we have already imported the TextLoader with the line `from langchain.document_loaders import TextLoader`. "
      ],
      "metadata": {
        "id": "UnyaykPx--Ug"
      },
      "id": "UnyaykPx--Ug"
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader('./state_of_the_union.txt', encoding='utf8')"
      ],
      "metadata": {
        "id": "LbjMScII-9wi"
      },
      "id": "LbjMScII-9wi",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quick Start: VectorStoreIndexCreator\n",
        "\n",
        "Langchain includes the class `VectorStoreIndexCreator`, which does exactly what the name suggests: it creates the index vector store for you. This function combines the Text Splitter, Vector Store, and Retriever all in one step. Just send it your document loader, and it will return the index created from that document. "
      ],
      "metadata": {
        "id": "7aZamSs6tUIQ"
      },
      "id": "7aZamSs6tUIQ"
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorstoreIndexCreator().from_loaders([loader])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQIgIIegtxTk",
        "outputId": "d2c08de1-ff6a-4402-d896-057b85bf47f8"
      },
      "id": "mQIgIIegtxTk",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will explore exactly what this function does later (and customize our own index), but for now, let's look at how we can use the index we've created. The VectorStoreIndexCreator comes with a method called `query` that lets us pass a prompt to the model."
      ],
      "metadata": {
        "id": "G0rz8yxMvG57"
      },
      "id": "G0rz8yxMvG57"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the `query` method to send a prompt to the model."
      ],
      "metadata": {
        "id": "O2AEffzDuoVq"
      },
      "id": "O2AEffzDuoVq"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What did the president say about Ketanji Brown Jackson\"\n",
        "index.query(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "x4eTTy6Runqy",
        "outputId": "a0eb9fc9-13a4-4119-887c-ced09dfa8b60"
      },
      "id": "x4eTTy6Runqy",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The president said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Detail: Text Splitter, Vector Store, and Retriever"
      ],
      "metadata": {
        "id": "CmSrchx1wMhB"
      },
      "id": "CmSrchx1wMhB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### First Step: Document Loader\n",
        "\n",
        "First, load your documents as you did previously."
      ],
      "metadata": {
        "id": "aRymA-y4wYXR"
      },
      "id": "aRymA-y4wYXR"
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader('./state_of_the_union.txt', encoding='utf8')"
      ],
      "metadata": {
        "id": "gtwBeUcCwes9"
      },
      "id": "gtwBeUcCwes9",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Step: Text Splitter"
      ],
      "metadata": {
        "id": "zs2onYCiAKjz"
      },
      "id": "zs2onYCiAKjz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to split up the text into smaller chunks, which will later be mapped to an embedding."
      ],
      "metadata": {
        "id": "qYxrOrdlwgEz"
      },
      "id": "qYxrOrdlwgEz"
    },
    {
      "cell_type": "code",
      "source": [
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "pm2oM_IyAPfK"
      },
      "id": "pm2oM_IyAPfK",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Third Step: Vector Store\n",
        "\n",
        "Now you create the Vector Store. We are using one called ChromaDB because it is free to use, and doesn't require an API. There are many other [Vector Stores](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html) worth looking up, however. "
      ],
      "metadata": {
        "id": "7KjUpzGFBDkw"
      },
      "id": "7KjUpzGFBDkw"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fZtUHarHA6HS"
      },
      "id": "fZtUHarHA6HS",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We aren't actually sending the text itself to the vector store. Instead, we send it the model embeddings. In this case, we want to use the OpenAI embeddings. Chroma allows us to upload the embeddings by passing it the split texts we've created as well as the embedding function."
      ],
      "metadata": {
        "id": "3k5mKmt8xBF-"
      },
      "id": "3k5mKmt8xBF-"
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "Y7xGl0DzAygv"
      },
      "id": "Y7xGl0DzAygv",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "db = Chroma.from_documents(texts, embeddings)"
      ],
      "metadata": {
        "id": "VTJKDmChBAGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161af210-e082-473f-8aa4-4474295015b1"
      },
      "id": "VTJKDmChBAGS",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fourth Step: Retriever"
      ],
      "metadata": {
        "id": "7DrGAyEPBGky"
      },
      "id": "7DrGAyEPBGky"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to expose our database as a retriever so that our model can find relevant information efficiently."
      ],
      "metadata": {
        "id": "XZjLWCJWxWfn"
      },
      "id": "XZjLWCJWxWfn"
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "EDWcKL67BNCX"
      },
      "id": "EDWcKL67BNCX",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fifth Step: Create a chain to use our Index!"
      ],
      "metadata": {
        "id": "35krJrGvBUhq"
      },
      "id": "35krJrGvBUhq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to choose a chain to interact with our index. A useful chain for asking questions over a Vector Store is the RetrievalQA chain. "
      ],
      "metadata": {
        "id": "bBbHJtcIxkJs"
      },
      "id": "bBbHJtcIxkJs"
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)"
      ],
      "metadata": {
        "id": "j2gRWyq0BbKJ"
      },
      "id": "j2gRWyq0BbKJ",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "id": "OMuMrYagBgUY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "eebfd6fd-b44c-4c54-db1f-7d84b2291a31"
      },
      "id": "OMuMrYagBgUY",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The president said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, VectorStoreIndexCreator wrapped up a lot of the hard work for us! You can see there are a lot of individual parts that can be customized for this process. "
      ],
      "metadata": {
        "id": "2z3bSbMTxt6d"
      },
      "id": "2z3bSbMTxt6d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finally: AI-assisted Programming with Langchain\n",
        "\n",
        "We  have covered a lot in this notebook! You could easily spend hours reading all the documentation on Langchain. This is where we could use some AI-assisted programming! Unfortunately, most of the LLMs (like Bard, BingGPT, or ChatGPT) don't have recent enough information (as of writing this notebook...) to know about Langchain.\n",
        "\n",
        "*But*... what if we could *use Langchain to solve this problem*?? \n",
        "\n",
        "**Class Question**: What could be a good workflow to create a bot that could answer questions about Langchain? Tell me your ideas!!"
      ],
      "metadata": {
        "id": "sqfRhzJosCZD"
      },
      "id": "sqfRhzJosCZD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### REVEAL ANSWER"
      ],
      "metadata": {
        "id": "rXwMqfxtCgnc"
      },
      "id": "rXwMqfxtCgnc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "How about this:\n",
        "1. Use Langchain to create an Index of Langchain documentation.\n",
        "2. Implement a QA chain to ask questions about the documentation.\n",
        "\n",
        "*In fact, we already did this! (And so did Langchain...)*"
      ],
      "metadata": {
        "id": "MX8_iLs9ClXE"
      },
      "id": "MX8_iLs9ClXE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introducing: Langchain Assistant"
      ],
      "metadata": {
        "id": "Ly54u5WKDN2X"
      },
      "id": "Ly54u5WKDN2X"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We at the DSI have created a bot to answer Langchain questions and it hosted on HuggingFace: [try it out](https://huggingface.co/spaces/vanderbilt-dsi/langchain-assistant).\n",
        "\n",
        "Because Langchain is so new and very fast-paced, Langchain developers have also created a chatbot for Langchain documentation (and yes, it's also built with Langchain). To access their bot, go to the [Langchain Documentation](https://python.langchain.com/en/latest/index.html) and click the parrot emoji at the bottom right (or type CTRL-K/CMD-K)."
      ],
      "metadata": {
        "id": "OL6FSQN4DPFS"
      },
      "id": "OL6FSQN4DPFS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Breakout Exercise (10 mins)\n",
        "Try out either the DSI's Langchain Assistant, or Langchain's own version on their website. What tasks would you like to do with Langchain? Does the bot produce code/answers that are helpful? Does one bot perform better than the other (don't worry, you won't hurt our feelings).\n",
        "\n",
        "*Note: these bots work best with a prompt along the lines of \"write Python code that uses Langchain to {do something}\"*\n",
        "\n",
        "If you have time, look at the underlying code for one of these bots ([DSI app](https://huggingface.co/spaces/vanderbilt-dsi/langchain-assistant/blob/main/app.py) or Chat Langchain's [document loader](https://github.com/hwchase17/chat-langchain/blob/master/ingest.py) and [data query](https://github.com/hwchase17/chat-langchain/blob/master/query_data.py). What do you notice about it? Do you recognize any of the concepts we've talked about at AI Summer?"
      ],
      "metadata": {
        "id": "o9DMdJGmEgEl"
      },
      "id": "o9DMdJGmEgEl"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "CmSrchx1wMhB",
        "rXwMqfxtCgnc"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}